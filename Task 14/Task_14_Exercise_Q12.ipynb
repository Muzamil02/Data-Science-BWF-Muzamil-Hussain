{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObD1nzERhcc8",
        "outputId": "314c9428-7fb2-4377-b4f3-46111edae791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.6817859304548384\n",
            "Epoch 10, Loss: 1.628480953990373\n",
            "Epoch 20, Loss: 1.5796805155007905\n",
            "Epoch 30, Loss: 1.535215639901795\n",
            "Epoch 40, Loss: 1.4948728023419178\n",
            "Epoch 50, Loss: 1.458405248834145\n",
            "Epoch 60, Loss: 1.4255444611825523\n",
            "Epoch 70, Loss: 1.396010840780338\n",
            "Epoch 80, Loss: 1.3695229693406223\n",
            "Epoch 90, Loss: 1.3458051039228396\n",
            "Epoch 100, Loss: 1.3245928238657652\n",
            "Epoch 110, Loss: 1.3056369393839782\n",
            "Epoch 120, Loss: 1.288705889712266\n",
            "Epoch 130, Loss: 1.2735869123057537\n",
            "Epoch 140, Loss: 1.2600862703869111\n",
            "Epoch 150, Loss: 1.2480288015980923\n",
            "Epoch 160, Loss: 1.237257010453709\n",
            "Epoch 170, Loss: 1.227629882399196\n",
            "Epoch 180, Loss: 1.2190215543167984\n",
            "Epoch 190, Loss: 1.2113199388380085\n",
            "Epoch 200, Loss: 1.2044253691793414\n",
            "Epoch 210, Loss: 1.1982493073966365\n",
            "Epoch 220, Loss: 1.192713141214395\n",
            "Epoch 230, Loss: 1.1877470818986202\n",
            "Epoch 240, Loss: 1.1832891669368601\n",
            "Epoch 250, Loss: 1.1792843656000334\n",
            "Epoch 260, Loss: 1.1756837819754242\n",
            "Epoch 270, Loss: 1.1724439481330264\n",
            "Epoch 280, Loss: 1.169526199224425\n",
            "Epoch 290, Loss: 1.1668961221491314\n",
            "Epoch 300, Loss: 1.1645230696950666\n",
            "Epoch 310, Loss: 1.162379732584004\n",
            "Epoch 320, Loss: 1.1604417625044818\n",
            "Epoch 330, Loss: 1.1586874399120466\n",
            "Epoch 340, Loss: 1.1570973810680867\n",
            "Epoch 350, Loss: 1.1556542794434674\n",
            "Epoch 360, Loss: 1.1543426772153098\n",
            "Epoch 370, Loss: 1.1531487631276134\n",
            "Epoch 380, Loss: 1.1520601934678572\n",
            "Epoch 390, Loss: 1.1510659333346898\n",
            "Epoch 400, Loss: 1.1501561157406748\n",
            "Epoch 410, Loss: 1.1493219164141009\n",
            "Epoch 420, Loss: 1.1485554424406215\n",
            "Epoch 430, Loss: 1.1478496331243242\n",
            "Epoch 440, Loss: 1.1471981716537838\n",
            "Epoch 450, Loss: 1.1465954063362391\n",
            "Epoch 460, Loss: 1.146036280316266\n",
            "Epoch 470, Loss: 1.1455162688277252\n",
            "Epoch 480, Loss: 1.1450313231423361\n",
            "Epoch 490, Loss: 1.1445778204776116\n",
            "Epoch 500, Loss: 1.1441525192132207\n",
            "Epoch 510, Loss: 1.143752518840122\n",
            "Epoch 520, Loss: 1.1433752241324824\n",
            "Epoch 530, Loss: 1.1430183130899365\n",
            "Epoch 540, Loss: 1.1426797082482079\n",
            "Epoch 550, Loss: 1.1423575510005097\n",
            "Epoch 560, Loss: 1.1420501786112855\n",
            "Epoch 570, Loss: 1.1417561036383947\n",
            "Epoch 580, Loss: 1.141473995510473\n",
            "Epoch 590, Loss: 1.141202664033276\n",
            "Epoch 600, Loss: 1.1409410446229404\n",
            "Epoch 610, Loss: 1.140688185085469\n",
            "Epoch 620, Loss: 1.1404432337808417\n",
            "Epoch 630, Loss: 1.1402054290271362\n",
            "Epoch 640, Loss: 1.1399740896152093\n",
            "Epoch 650, Loss: 1.1397486063180275\n",
            "Epoch 660, Loss: 1.1395284342908476\n",
            "Epoch 670, Loss: 1.1393130862692615\n",
            "Epoch 680, Loss: 1.1391021264818058\n",
            "Epoch 690, Loss: 1.138895165202512\n",
            "Epoch 700, Loss: 1.138691853876524\n",
            "Epoch 710, Loss: 1.1384918807588735\n",
            "Epoch 720, Loss: 1.1382949670127314\n",
            "Epoch 730, Loss: 1.1381008632190306\n",
            "Epoch 740, Loss: 1.1379093462543701\n",
            "Epoch 750, Loss: 1.137720216498586\n",
            "Epoch 760, Loss: 1.1375332953374022\n",
            "Epoch 770, Loss: 1.1373484229291735\n",
            "Epoch 780, Loss: 1.137165456207968\n",
            "Epoch 790, Loss: 1.1369842670981318\n",
            "Epoch 800, Loss: 1.1368047409180704\n",
            "Epoch 810, Loss: 1.1366267749533154\n",
            "Epoch 820, Loss: 1.1364502771810256\n",
            "Epoch 830, Loss: 1.1362751651299428\n",
            "Epoch 840, Loss: 1.136101364861494\n",
            "Epoch 850, Loss: 1.1359288100592402\n",
            "Epoch 860, Loss: 1.1357574412152072\n",
            "Epoch 870, Loss: 1.135587204902843\n",
            "Epoch 880, Loss: 1.1354180531274283\n",
            "Epoch 890, Loss: 1.1352499427457223\n",
            "Epoch 900, Loss: 1.1350828349475055\n",
            "Epoch 910, Loss: 1.1349166947924436\n",
            "Epoch 920, Loss: 1.1347514907963974\n",
            "Epoch 930, Loss: 1.13458719456192\n",
            "Epoch 940, Loss: 1.134423780448244\n",
            "Epoch 950, Loss: 1.134261225276553\n",
            "Epoch 960, Loss: 1.1340995080667786\n",
            "Epoch 970, Loss: 1.1339386098025657\n",
            "Epoch 980, Loss: 1.1337785132214013\n",
            "Epoch 990, Loss: 1.1336192026272212\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "def compute_loss_and_gradients(X, y, theta):\n",
        "    m = X.shape[0]\n",
        "    logits = X.dot(theta)\n",
        "    y_proba = softmax(logits)\n",
        "    loss = -np.mean(np.log(y_proba[np.arange(m), y]))\n",
        "    y_one_hot = np.zeros_like(y_proba)\n",
        "    y_one_hot[np.arange(m), y] = 1\n",
        "    gradients = (1 / m) * X.T.dot(y_proba - y_one_hot)\n",
        "    return loss, gradients\n",
        "\n",
        "def batch_gradient_descent(X, y, learning_rate=0.01, n_epochs=1000, batch_size=32, early_stopping_rounds=10):\n",
        "    m, n = X.shape\n",
        "    n_classes = len(np.unique(y))\n",
        "    theta = np.random.randn(n, n_classes)\n",
        "    best_loss = np.inf\n",
        "    epochs_no_improvement = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss, gradients = compute_loss_and_gradients(X, y, theta)\n",
        "        theta -= learning_rate * gradients\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            epochs_no_improvement = 0\n",
        "        else:\n",
        "            epochs_no_improvement += 1\n",
        "\n",
        "        if epochs_no_improvement > early_stopping_rounds:\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "    return theta\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(1000, 3)  # 1000 instances, 3 features\n",
        "y = np.random.randint(0, 3, 1000)  # 3 classes\n",
        "\n",
        "X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "theta = batch_gradient_descent(X_bias, y)\n",
        "print(\"Training complete.\")"
      ]
    }
  ]
}